{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qR97M5igjfyh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LihUvVy0oOlv"},"outputs":[],"source":["!unzip -q /content/drive/MyDrive/face_age.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZiOXV9_oUev"},"outputs":[],"source":["from tensorflow.keras import mixed_precision\n","mixed_precision.set_global_policy('mixed_float16')\n","\n","import tensorflow as tf\n","\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_memory_growth(gpus[0], True)\n","        tf.config.set_visible_devices(gpus[0], 'GPU')\n","        print(\"GPU is set and ready!\")\n","    except RuntimeError as e:\n","\n","        print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"249mLNQtojQl"},"outputs":[],"source":["\n","import numpy as np\n","import os\n","import pandas as pd\n","import kagglehub\n","import tensorflow as tf\n","import cv2\n","import imghdr\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import Precision, Recall, Accuracy\n","from matplotlib import pyplot as plt\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.losses import MeanSquaredError\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Add, UpSampling2D, Layer\n","from tensorflow.keras.layers import Activation, GlobalAveragePooling2D, Dropout\n","from tensorflow.keras.models import load_model, Model"]},{"cell_type":"code","source":["tf.random.set_seed(42)"],"metadata":{"id":"zeSzBFUNCwoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVnh9LHnoydM"},"outputs":[],"source":["data_dir = 'face_age'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jyGM6teo_D5"},"outputs":[],"source":["image_exts = [\"jpeg\",\"jpg\",\"bmp\",\"png\"]\n","for image_class in os.listdir(data_dir):\n","    class_path = os.path.join(data_dir, image_class)\n","    if not os.path.isdir(class_path):\n","        continue\n","\n","    for image in os.listdir(class_path):\n","        image_path = os.path.join(class_path, image)\n","        try:\n","            img = cv2.imread(image_path)\n","            tip = imghdr.what(image_path)\n","\n","            if tip not in image_exts:\n","                print(f\"Not a valid image: {image_path}\")\n","                os.remove(image_path)\n","        except Exception as e:\n","            print(f\"Error processing {image_path}: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oR5_4Gh2pFzr"},"outputs":[],"source":["import glob\n","for ds_store_file in glob.glob(f\"{data_dir}/**/.DS_Store\", recursive=True):\n","    os.remove(ds_store_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e_uyYZlpGTJ"},"outputs":[],"source":["\n","class_names = sorted(os.listdir(data_dir))\n","age_labels = [int(name) for name in class_names]  # Convert subdir names to integers\n","\n","# lookup table for class index â†’ actual age\n","age_lookup = tf.constant(age_labels, dtype=tf.int32)\n","\n","# 3. Map dataset labels to actual ages\n","data = tf.keras.utils.image_dataset_from_directory(data_dir)\n","data = data.map(lambda x, y: (x, tf.gather(age_lookup, y)))\n","\n","# 4. Now apply your age-to-category mapping\n","def label_to_category(image, label):\n","    category = tf.where(\n","        label < 13, 0,\n","        tf.where(\n","            label < 20, 1,\n","            tf.where(label < 60, 2, 3)\n","        )\n","    )\n","    return image, tf.cast(category, tf.int32)\n","\n","dataset = data.map(label_to_category)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w16lg1pbpBLn"},"outputs":[],"source":["dataset = dataset.map(lambda x,y:(x/255,y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZuLYuRinpNmo"},"outputs":[],"source":["scaled_iterator_1 = dataset.as_numpy_iterator()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFDAYaaIpRFG"},"outputs":[],"source":["batch_dataset = scaled_iterator_1.next()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmFdD8KBphXs"},"outputs":[],"source":["length = dataset.cardinality().numpy()\n","\n","train_size_d = int(length * 0.7)\n","val_size_d = int(length * 0.2)\n","test_size_d = length - train_size_d - val_size_d\n","\n","train_dataset = dataset.take(train_size_d)\n","val_dataset = dataset.skip(train_size_d).take(val_size_d)\n","test_dataset = dataset.skip(train_size_d + val_size_d)\n"]},{"cell_type":"markdown","metadata":{"id":"Kca3BtFipnz6"},"source":["### **Block 1 and Block 2  Splitting Training set**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3kET1bcqCnQ"},"outputs":[],"source":["split_point_1 = train_size_d // 2\n","\n","# Create the first set of training and validation datasets\n","block_train_1 = train_dataset.take(split_point_1)\n","block_val_1 = val_dataset.take(val_size_d // 2)\n","\n","# Create the second set of training and validation datasets\n","block_train_2 = train_dataset.skip(split_point_1)\n","block_val_2 = val_dataset.skip(val_size_d // 2)\n","\n","print(\"Block 1 Training dataset size:\", block_train_1.cardinality().numpy())\n","print(\"Block 1 Validation dataset size:\", block_val_1.cardinality().numpy())\n","print(\"Block 2 Training dataset size:\", block_train_2.cardinality().numpy())\n","print(\"Block 2 Validation dataset size:\", block_val_2.cardinality().numpy())\n"]},{"cell_type":"code","source":["for x, y in block_train_1.take(1):\n","    print(\"Training x shape:\", x.shape)\n","    print(\"Training y shape:\", y.shape)\n","\n","for x, y in block_val_1.take(1):\n","    print(\"Validation x shape:\", x.shape)\n","    print(\"Validation y shape:\", y.shape)"],"metadata":{"id":"wWPunc---oXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mobilenet_train_1 = block_train_1\n","mobilenet_val_1 = block_val_1\n"],"metadata":{"id":"-1IQJZAt_AFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x, y in mobilenet_train_1.take(1):\n","    print(\"Training x shape:\", x.shape)\n","    print(\"Training y shape:\", y.shape)\n","\n","for x, y in block_val_1.take(1):\n","    print(\"Validation x shape:\", x.shape)\n","    print(\"Validation y shape:\", y.shape)"],"metadata":{"id":"irYWqVIa_CPP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ol_86M5nqauv"},"source":["# **Part 1  Auto encoder**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZO9XP1qw0O0"},"outputs":[],"source":["input_img = Input(shape=(256, 256, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXk78ZiPqXZG"},"outputs":[],"source":["\n","# encodes the image\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n","x = MaxPooling2D((2, 2), padding='same')(x)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","encoded = MaxPooling2D((2, 2), padding='same')(x)\n","\n","#  Decoder(it tries to re- construct the encoded image)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n","x = UpSampling2D((2, 2))(x)\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n","x = UpSampling2D((2, 2))(x)\n","decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n","\n","# autoencoder Model\n","autoencoder = Model(input_img, decoded)\n","autoencoder.compile(optimizer=Adam(), loss=MeanSquaredError())\n","\n","# data\n","block_train_1 = block_train_1.map(lambda x, y: (x, x))\n","block_val_1 = block_val_1.map(lambda x, y: (x, x))\n","\n","# training the autoencoder\n","history = autoencoder.fit(\n","    block_train_1,\n","    epochs=30,\n","    batch_size=64,\n","    shuffle=True,\n","    validation_data=block_val_1\n",")\n","\n","autoencoder.save(\"autoencoder_block1_model.h5\")"]},{"cell_type":"markdown","source":["# **Part 2 Build Classification model on the Auto-encoder**"],"metadata":{"id":"GbFnTOXsXPjd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVqAfnpOyIwf"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import load_model, Model\n","from tensorflow.keras.layers import Layer\n","\n","# Define custom Cast layer\n","class Cast(Layer):\n","    def __init__(self, dtype, **kwargs):\n","        super(Cast, self).__init__(**kwargs)\n","        self._dtype = dtype\n","\n","    def call(self, inputs):\n","        return tf.cast(inputs, self._dtype)\n","\n","    def get_config(self):\n","        config = super(Cast, self).get_config()\n","        config.update({\"dtype\": self._dtype})\n","        return config\n","\n","# Define custom TypeConverter layer\n","class TypeConverter(Layer):\n","    def __init__(self, data_type, **kwargs):\n","        super(TypeConverter, self).__init__(**kwargs)\n","        self._data_type = data_type\n","\n","    def call(self, inputs):\n","        return tf.cast(inputs, self._data_type)\n","\n","    def get_config(self):\n","        config = super(TypeConverter, self).get_config()\n","        config.update({\"data_type\": self._data_type})\n","        return config\n","\n","# Load model with custom layers\n","autoencoder = load_model(\n","    \"autoencoder_block1_model.h5\",\n","    custom_objects={'TypeConverter': TypeConverter, 'Cast': Cast}\n",")\n","\n","# Extract encoder\n","feature_extractor = Model(\n","    inputs=autoencoder.input,\n","    outputs=autoencoder.layers[-2].output\n",")\n","\n","# Freeze encoder layers\n","for layer in feature_extractor.layers:\n","    layer.trainable = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEPUHiM_4NZB"},"outputs":[],"source":["# Classification Model\n","features = feature_extractor.output\n","features = GlobalAveragePooling2D()(features)\n","features = Dense(1024, activation='relu')(features)  # A layer to learn complex patterns\n","predictions = Dense(4, activation='softmax')(features)  # Softmax gives probabilities\n","age_classifier = Model(inputs=feature_extractor.input, outputs=predictions)\n","\n","\n","age_classifier.compile(\n","    optimizer=Adam(learning_rate=0.0001),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80E8hoJuyllx"},"outputs":[],"source":["training_history = age_classifier.fit(block_train_2, epochs=10, batch_size=32,validation_data=block_val_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeiF3mol4U9J"},"outputs":[],"source":["\n","test_loss_1, test_accuracy_1 = age_classifier.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss_1:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy_1:.4f}\")"]},{"cell_type":"code","source":["plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(training_history.history['loss'], label='Training Loss')\n","plt.plot(training_history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(training_history.history['accuracy'], label='Training Accuracy')\n","plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"VS7AXmgKbujp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Fine tuning the model**\n"],"metadata":{"id":"nXRstyYiZzqU"}},{"cell_type":"markdown","source":["\n","\n","*   Increasing Learning rate\n","*   Number of nodes to learn complex patterns\n","*   unfreezing the encoder layers\n","*   Changed activation to tan-h\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"uKTsdFJnf0X6"}},{"cell_type":"code","source":["# Freeze encoder layers\n","for layer in feature_extractor.layers:\n","    layer.trainable = True"],"metadata":{"id":"IPQwmWkmgLo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Classification Model(fine tuning)\n","features = Dense(2048, activation='tanh')(features)  # A layer to learn complex patterns\n","predictions_1 = Dense(4, activation='softmax')(features)  # Softmax gives probabilities\n","age_classifier_1 = Model(inputs=feature_extractor.input, outputs=predictions_1)\n","\n","\n","age_classifier_1.compile(\n","    optimizer=Adam(learning_rate = 0.0003),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")"],"metadata":{"id":"h5OkLP-sZmf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_history = age_classifier_1.fit(block_train_2, epochs=10, batch_size=32,validation_data=block_val_2)"],"metadata":{"id":"iCFJknyJaQPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","test_loss_2, test_accuracy_2 = age_classifier_1.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss_2:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy_2:.4f}\")"],"metadata":{"id":"unVHA_lScvqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **2nd optimization**"],"metadata":{"id":"5LiSCNLyp4IN"}},{"cell_type":"markdown","source":["\n","\n","*   Increasing Learning rate\n","*   Changed activation to relu again\n","*   Increase Epochs\n","*   using Flatten\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"4c3mxTtCgsbF"}},{"cell_type":"code","source":["\n","# Classification Model(fine tuning)\n","features = Flatten()(features)\n","features = Dense(2048, activation='relu')(features)  # A layer to learn complex patterns\n","predictions_2 = Dense(4, activation='softmax')(features)  # Softmax gives probabilities\n","age_classifier_2 = Model(inputs=feature_extractor.input, outputs=predictions_2)\n","\n","\n","age_classifier_2.compile(\n","    optimizer=Adam(learning_rate = 0.0005),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")"],"metadata":{"id":"UrdL4_WCdRE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_history = age_classifier_2.fit(block_train_2, epochs=20, batch_size=32,validation_data=block_val_2)"],"metadata":{"id":"_s-ca7OViKPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","test_loss_3, test_accuracy_3 = age_classifier_2.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss_3:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy_3:.4f}\")"],"metadata":{"id":"la_a_KzeiMpl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Last optimization**"],"metadata":{"id":"14OMMYb9o7LZ"}},{"cell_type":"markdown","source":["\n","*   Decreasing nodes in layer\n","\n"],"metadata":{"id":"fsi6zjsPp0_K"}},{"cell_type":"code","source":["features = Flatten()(features)\n","features = Dense(64, activation='relu')(features)\n","features = Dense(128, activation='relu')(features)\n","features = Dense(264, activation='relu')(features)\n","features = Dense(1024, activation='relu')(features)\n","predictions_3 = Dense(4, activation='softmax')(features)  # Softmax gives probabilities\n","age_classifier_3 = Model(inputs=feature_extractor.input, outputs=predictions_3)\n","\n","age_classifier_3.compile(\n","    optimizer=Adam(learning_rate = 0.0005),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")"],"metadata":{"id":"wgrihYeai1ty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","training_history = age_classifier_3.fit(block_train_2, epochs=10, batch_size=32,validation_data=block_val_2)"],"metadata":{"id":"t5r5d3EEqP86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","test_loss_4, test_accuracy_4 = age_classifier_3.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss_4:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy_4:.4f}\")"],"metadata":{"id":"j1fxFZZrqbfa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **PreTrained Mobile net model**"],"metadata":{"id":"0fM_8B2J_itQ"}},{"cell_type":"code","source":["from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras import layers, models\n","\n","# Load base MobileNetV2 without top layer, accepting 256x256 input\n","base_model = MobileNetV2(\n","    input_shape=(256, 256, 3),  # Changed input shape to 256x256\n","    include_top=False,\n","    weights='imagenet'\n",")\n","base_model.trainable = False  # Freeze initial layers\n","\n","inputs = Input(shape=(256, 256, 3))  # Input shape changed here as well\n","x = base_model(inputs, training=False)\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.BatchNormalization()(x)\n","x = layers.Dense(128, activation='relu')(x)\n","x = layers.Dropout(0.3)(x)\n","outputs = layers.Dense(4, activation='softmax')(x)\n","\n","mobilenet_model = models.Model(inputs, outputs)\n","\n","mobilenet_model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","mobilenet_model.summary()"],"metadata":{"id":"Tymo1222ue-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Train**"],"metadata":{"id":"sE5E60Rq_qiz"}},{"cell_type":"code","source":["mobilenet = mobilenet_model.fit(\n","    mobilenet_train_1 ,\n","    validation_data= mobilenet_val_1,\n","    epochs=20,\n",")"],"metadata":{"id":"qa3gPtZT62gL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Test**"],"metadata":{"id":"eEiTiazo_ud1"}},{"cell_type":"code","source":["test_loss, test_acc = mobilenet_model.evaluate(test_dataset)\n","print(f\"MobileNetV2 Test Accuracy: {test_acc:.2%}\")"],"metadata":{"id":"0O0W5BIU-Gk-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Comparing Model**"],"metadata":{"id":"iNDbABhv_zwa"}},{"cell_type":"code","source":["model_names = ['Autoencoder Model 1', 'Autoencoder Model 2', 'Autoencoder Model 3', 'Autoencoder Model 4', 'MobileNetV2']\n","accuracies = [test_accuracy_1, test_accuracy_2, test_accuracy_3, test_accuracy_4, test_acc]\n","\n","# Find the model with the highest accuracy\n","best_model_index = np.argmax(accuracies)\n","best_model_name = model_names[best_model_index]\n","best_accuracy = accuracies[best_model_index]\n","\n","print(f\"The best performing model is {best_model_name} with an accuracy of {best_accuracy:.4f}\")\n","\n","# Plotting the accuracy graph\n","plt.figure(figsize=(10, 6))\n","plt.plot(model_names, accuracies, marker='o')\n","plt.xlabel(\"Models\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Comparison of Model Accuracies\")\n","plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"hU8rf-QFFTlJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow import keras\n","\n","# Save regression model\n","age_classifier_3.save('age_classifier_3.keras')\n","\n","# Save classification model\n","mobilenet_model.save('mobilenet_model.keras')\n"],"metadata":{"id":"_Fqhs_lqFZsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","os.makedirs(\"part_2_models\", exist_ok=True)\n","shutil.move(\"age_classifier_3.keras\", \"part_2_models/age_classifier_3.keras\")\n","shutil.move(\"mobilenet_model.keras\", \"part_2_models/mobilenet_model.keras\")\n","shutil.make_archive(\"part_2_models\", 'zip', \"part_2_models\")\n"],"metadata":{"id":"XegcYgA73IlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eXY2eI-r3fdp"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPs8zVWlfRpmPTv7QhpMnBz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}